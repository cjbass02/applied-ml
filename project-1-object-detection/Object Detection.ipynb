{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940d05b3",
   "metadata": {},
   "source": [
    "# Object Detection with YOLO\n",
    "\n",
    "This notebook provides a ready-to-run setup of the You Only Look Once (YOLO) v3 network for object detection. The [YOLO family of models](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html) were created by [Joseph Chet Redmon](https://pjreddie.com/).  Training these models requires large data sets like [ImageNet]() and [Microsoft COCO]() and significant compute resources, making it infeasible for most users to train their own models.  Thankfully, the researchers have released the weights of their trained models and the community has developed code that allows these models to be run with frameworks like Pytorch and Tensorflow.  This notebook uses [code](https://github.com/experiencor/keras-yolo3) released under the MIT license by [Huynh Ngoc Anh](https://github.com/experiencor) to run the pre-trained YOLO model in Keras.\n",
    "\n",
    "To use this notebook, you will need to download the YOLOv3 model weights from [https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights).  Place the `yolov3.weights` file in the same directory as this notebook.\n",
    "\n",
    "You will need to change the `input_image_path` and `output_image_path` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4888582e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T19:55:27.969653Z",
     "iopub.status.busy": "2026-02-08T19:55:27.969311Z",
     "iopub.status.idle": "2026-02-08T19:55:28.122290Z",
     "shell.execute_reply": "2026-02-08T19:55:28.121223Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b12a597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T19:55:28.125345Z",
     "iopub.status.busy": "2026-02-08T19:55:28.124903Z",
     "iopub.status.idle": "2026-02-08T19:55:28.708688Z",
     "shell.execute_reply": "2026-02-08T19:55:28.707703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "weights_path = \"yolov3.weights\"\n",
    "config_path = \"yolov3.cfg\"\n",
    "output_dir = \"outputs\"\n",
    "Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Supports .png, .jpg, and .jpeg files\n",
    "included_images = [\n",
    "    \"IMG_1703.jpeg\",\n",
    "    \"IMG_1704.jpeg\",\n",
    "    \"IMG_4134.jpeg\",\n",
    "    \"IMG_5415.jpg\",\n",
    "    \"IMG_7207.jpg\",\n",
    "]\n",
    "\n",
    "# Optional: download a couple of web images to use as \"own\" images\n",
    "web_images = {\n",
    "    \"web_dog.jpg\": \"https://raw.githubusercontent.com/pjreddie/darknet/master/data/dog.jpg\",\n",
    "    \"web_giraffe.jpg\": \"https://raw.githubusercontent.com/pjreddie/darknet/master/data/giraffe.jpg\",\n",
    "}\n",
    "\n",
    "for filename, url in web_images.items():\n",
    "    if not Path(filename).exists():\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "own_images = list(web_images.keys())\n",
    "\n",
    "net_h, net_w = 416, 416\n",
    "obj_thresh, nms_thresh = 0.5, 0.45\n",
    "\n",
    "labels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n",
    "          \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n",
    "          \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n",
    "          \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n",
    "          \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n",
    "          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n",
    "          \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n",
    "          \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n",
    "          \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n",
    "          \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a853b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T19:55:28.712233Z",
     "iopub.status.busy": "2026-02-08T19:55:28.711853Z",
     "iopub.status.idle": "2026-02-08T19:55:29.493807Z",
     "shell.execute_reply": "2026-02-08T19:55:29.492808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load YOLOv3 model via OpenCV DNN\n",
    "if not Path(weights_path).exists():\n",
    "    raise FileNotFoundError(f\"Missing weights file: {weights_path}\")\n",
    "if not Path(config_path).exists():\n",
    "    raise FileNotFoundError(f\"Missing config file: {config_path}\")\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "output_layer_names = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "\n",
    "def detect_and_draw(image_path, output_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not read image: {image_path}\")\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image, 1 / 255.0, (net_w, net_h), swapRB=True, crop=False\n",
    "    )\n",
    "\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(output_layer_names)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = int(np.argmax(scores))\n",
    "            confidence = float(scores[class_id])\n",
    "            if confidence > obj_thresh:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(confidence)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, obj_thresh, nms_thresh)\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = labels[class_ids[i]] if class_ids[i] < len(labels) else str(class_ids[i])\n",
    "            score = confidences[i]\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                f\"{label} {score:.2f}\",\n",
    "                (x, max(y - 10, 0)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 255, 0),\n",
    "                2,\n",
    "            )\n",
    "\n",
    "    cv2.imwrite(output_path, image)\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"output_path\": output_path,\n",
    "        \"detections\": [\n",
    "            {\n",
    "                \"label\": labels[class_ids[i]] if class_ids[i] < len(labels) else str(class_ids[i]),\n",
    "                \"confidence\": confidences[i],\n",
    "                \"box\": boxes[i],\n",
    "            }\n",
    "            for i in (indices.flatten().tolist() if len(indices) > 0 else [])\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8400bb26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T19:55:29.496355Z",
     "iopub.status.busy": "2026-02-08T19:55:29.496110Z",
     "iopub.status.idle": "2026-02-08T19:55:33.989105Z",
     "shell.execute_reply": "2026-02-08T19:55:33.987907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_path': 'IMG_1703.jpeg',\n",
       "  'output_path': 'outputs\\\\IMG_1703_detected.jpeg',\n",
       "  'detections': [{'label': 'person',\n",
       "    'confidence': 0.9990772008895874,\n",
       "    'box': [587, 1217, 220, 660]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9962482452392578,\n",
       "    'box': [2517, 1354, 186, 405]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9955952167510986,\n",
       "    'box': [3720, 1324, 241, 791]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9912199378013611,\n",
       "    'box': [9, 1304, 430, 705]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9796616435050964,\n",
       "    'box': [2356, 1325, 179, 412]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9664183855056763,\n",
       "    'box': [1473, 1280, 185, 319]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.962527871131897,\n",
       "    'box': [3571, 1334, 183, 674]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9616000056266785,\n",
       "    'box': [2118, 1391, 151, 392]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9453965425491333,\n",
       "    'box': [1724, 1359, 144, 352]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.8515907526016235,\n",
       "    'box': [1441, 2266, 752, 750]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.8274660110473633,\n",
       "    'box': [2029, 2029, 512, 666]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.7338007688522339,\n",
       "    'box': [1231, 1797, 285, 318]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.7055687308311462,\n",
       "    'box': [983, 1864, 286, 363]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.6760959625244141,\n",
       "    'box': [451, 2053, 536, 373]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.6006314158439636,\n",
       "    'box': [1510, 1757, 212, 269]},\n",
       "   {'label': 'laptop',\n",
       "    'confidence': 0.5685692429542542,\n",
       "    'box': [3529, 2224, 481, 357]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.5182889103889465,\n",
       "    'box': [1932, 1738, 245, 197]}]},\n",
       " {'image_path': 'IMG_1704.jpeg',\n",
       "  'output_path': 'outputs\\\\IMG_1704_detected.jpeg',\n",
       "  'detections': [{'label': 'person',\n",
       "    'confidence': 0.995892345905304,\n",
       "    'box': [86, 214, 34, 67]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9846470952033997,\n",
       "    'box': [42, 205, 43, 74]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9806457161903381,\n",
       "    'box': [260, 217, 29, 98]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.9727472066879272,\n",
       "    'box': [286, 219, 41, 101]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.9723814129829407,\n",
       "    'box': [308, 340, 78, 90]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.7972885370254517,\n",
       "    'box': [242, 218, 26, 96]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.7761987447738647,\n",
       "    'box': [81, 288, 58, 68]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.6977071762084961,\n",
       "    'box': [59, 324, 54, 73]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.6680090427398682,\n",
       "    'box': [232, 209, 25, 107]},\n",
       "   {'label': 'chair',\n",
       "    'confidence': 0.5424807667732239,\n",
       "    'box': [263, 299, 35, 55]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.5170033574104309,\n",
       "    'box': [1, 206, 22, 93]}]},\n",
       " {'image_path': 'IMG_4134.jpeg',\n",
       "  'output_path': 'outputs\\\\IMG_4134_detected.jpeg',\n",
       "  'detections': [{'label': 'dog',\n",
       "    'confidence': 0.9945187568664551,\n",
       "    'box': [73, 204, 279, 339]},\n",
       "   {'label': 'book',\n",
       "    'confidence': 0.6331964731216431,\n",
       "    'box': [226, 1, 18, 40]},\n",
       "   {'label': 'book',\n",
       "    'confidence': 0.5924302339553833,\n",
       "    'box': [243, -1, 20, 38]},\n",
       "   {'label': 'book',\n",
       "    'confidence': 0.5867136716842651,\n",
       "    'box': [234, -1, 19, 40]}]},\n",
       " {'image_path': 'IMG_5415.jpg',\n",
       "  'output_path': 'outputs\\\\IMG_5415_detected.jpg',\n",
       "  'detections': [{'label': 'car',\n",
       "    'confidence': 0.9997216463088989,\n",
       "    'box': [1350, 1993, 1073, 445]},\n",
       "   {'label': 'car',\n",
       "    'confidence': 0.8410476446151733,\n",
       "    'box': [-6, 1973, 133, 539]}]},\n",
       " {'image_path': 'IMG_7207.jpg',\n",
       "  'output_path': 'outputs\\\\IMG_7207_detected.jpg',\n",
       "  'detections': [{'label': 'person',\n",
       "    'confidence': 0.9689112305641174,\n",
       "    'box': [1376, 2585, 101, 228]},\n",
       "   {'label': 'person',\n",
       "    'confidence': 0.7731399536132812,\n",
       "    'box': [1548, 2598, 145, 190]}]},\n",
       " {'image_path': 'web_dog.jpg',\n",
       "  'output_path': 'outputs\\\\web_dog_detected.jpg',\n",
       "  'detections': [{'label': 'dog',\n",
       "    'confidence': 0.9979028701782227,\n",
       "    'box': [122, 223, 197, 320]},\n",
       "   {'label': 'bicycle',\n",
       "    'confidence': 0.990092396736145,\n",
       "    'box': [117, 124, 452, 307]},\n",
       "   {'label': 'truck',\n",
       "    'confidence': 0.9369494915008545,\n",
       "    'box': [472, 86, 219, 79]}]},\n",
       " {'image_path': 'web_giraffe.jpg',\n",
       "  'output_path': 'outputs\\\\web_giraffe_detected.jpg',\n",
       "  'detections': [{'label': 'giraffe',\n",
       "    'confidence': 0.9975883960723877,\n",
       "    'box': [152, 48, 287, 385]},\n",
       "   {'label': 'zebra',\n",
       "    'confidence': 0.9575901031494141,\n",
       "    'box': [263, 199, 171, 251]}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for image_path in included_images + own_images:\n",
    "    output_path = os.path.join(\n",
    "        output_dir, f\"{Path(image_path).stem}_detected{Path(image_path).suffix}\"\n",
    "    )\n",
    "    results.append(detect_and_draw(image_path, output_path))\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
